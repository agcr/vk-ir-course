{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c68b6411-668d-431f-9556-9a478597aa6c",
   "metadata": {},
   "source": [
    "# Предобработка текста\n",
    "\n",
    "В этом тюториале мы научимся решать задачи токенизации, стемминга и лемматизации текста.\n",
    "\n",
    "Будем это делать с помощью трех популярных библиотек: NLTK, spaCY и pymorphy.\n",
    "\n",
    "Ссылки на документацию:\n",
    "- NLTK: https://www.nltk.org/\n",
    "- spaCy: https://spacy.io/\n",
    "- pymorphy: https://github.com/no-plagiarism/pymorphy3 (см. также https://github.com/pymorphy2/pymorphy2 -- этот проект был заброшен автором и форкнут авторами pymorphy3)\n",
    "\n",
    "Не будем подробно останавливаться на отличиях NLTK от spaCy, заметим только что в большинстве других примеров в данном курсе мы используем библиотеку NLTK."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fc459ca-2cd0-49bb-9670-461c981bffa9",
   "metadata": {},
   "source": [
    "## Токенизация и стемминг с использованием библиотеки NLTK"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f0a7027-7d31-480c-9e05-44b6ca966f68",
   "metadata": {},
   "source": [
    "Импортируем модули которые нам понадобятся впоследствии:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "21fb783d-937e-40a6-88b5-3df49b614a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import tokenize\n",
    "from nltk import stem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a26b075-1952-458c-b07c-d231b2202d90",
   "metadata": {},
   "source": [
    "Библиотека NLTK требует установки нескольких дополнительных моделей, загрузим их:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ab81a53d-3512-4e4b-8583-15c72fe6f876",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/andrei/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to /home/andrei/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61bd5f69-0f93-4a21-8aa6-62744c5e6476",
   "metadata": {},
   "source": [
    "Рассмотрим маленькую коллекцию из нескольких текстовых документов:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d84b812b-f9bd-41dd-96dc-3007b4efe5e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [\n",
    "    \"George Washington lives in Washington.\",\n",
    "    \"Thomas Jefferson lives in New York.\",\n",
    "    \"Джордж Вашингтон живет в Вашингтоне.\",\n",
    "    \"Томас Джефферсон живет в Нью-Йорке.\",\n",
    "    \"The U.S. are a country. The U.N. is an organization.\",\n",
    "    \"It's obvious!\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d8eed21-bb25-436c-88f3-4c8008c553fc",
   "metadata": {},
   "source": [
    "Попробуем их токенизировать.\n",
    "\n",
    "Простейший способ это сделать -- воспользоваться функцией _word_tokenize_:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "caf22b01-c63a-4bc4-936f-837471d14d53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['George', 'Washington', 'lives', 'in', 'Washington', '.']\n",
      "['Thomas', 'Jefferson', 'lives', 'in', 'New', 'York', '.']\n",
      "['Джордж', 'Вашингтон', 'живет', 'в', 'Вашингтоне', '.']\n",
      "['Томас', 'Джефферсон', 'живет', 'в', 'Нью-Йорке', '.']\n",
      "['The', 'U.S.', 'are', 'a', 'country', '.', 'The', 'U.N.', 'is', 'an', 'organization', '.']\n",
      "['It', \"'s\", 'obvious', '!']\n"
     ]
    }
   ],
   "source": [
    "for text in texts:\n",
    "    print(tokenize.word_tokenize(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e781ffb-e70a-4db2-b4c2-a450248fd6ac",
   "metadata": {},
   "source": [
    "Мы видим, что _word_tokenize_ позволяет делать умную токенизацию, которая, например, понимает что U.S. -- это страна, которая должна быть представлены в виде одного токена.\n",
    "\n",
    "На практике, нам, как правило, нужна не \"умная\" токенизация, а, скорее наоборот, что-то простое и над чем мы имеем максимальный контроль. Для этого хорошо подходит класс _RegexpTokenizer_.\n",
    "\n",
    "С его помощью можно, например, просто посплитить по пробелам и пунктуации:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0a39ec59-b88c-4ebe-87bd-2b8da51b1746",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['George', 'Washington', 'lives', 'in', 'Washington']\n",
      "['Thomas', 'Jefferson', 'lives', 'in', 'New', 'York']\n",
      "['Джордж', 'Вашингтон', 'живет', 'в', 'Вашингтоне']\n",
      "['Томас', 'Джефферсон', 'живет', 'в', 'Нью', 'Йорке']\n",
      "['The', 'U', 'S', 'are', 'a', 'country', 'The', 'U', 'N', 'is', 'an', 'organization']\n",
      "['It', 's', 'obvious']\n"
     ]
    }
   ],
   "source": [
    "tokenizer = tokenize.RegexpTokenizer(r'\\w+')\n",
    "for text in texts:\n",
    "    print(tokenizer.tokenize(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2678b98a-4202-49e8-ba67-ef571798cbdc",
   "metadata": {},
   "source": [
    "Аналогично, только осталяем слова длиной 2 или более символов:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "00d11d18-5690-41d9-ab7b-3ef99fce9a20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['George', 'Washington', 'lives', 'in', 'Washington']\n",
      "['Thomas', 'Jefferson', 'lives', 'in', 'New', 'York']\n",
      "['Джордж', 'Вашингтон', 'живет', 'Вашингтоне']\n",
      "['Томас', 'Джефферсон', 'живет', 'Нью', 'Йорке']\n",
      "['The', 'are', 'country', 'The', 'is', 'an', 'organization']\n",
      "['It', 'obvious']\n"
     ]
    }
   ],
   "source": [
    "tokenizer = tokenize.RegexpTokenizer(r'\\w\\w+')\n",
    "for text in texts:\n",
    "    print(tokenizer.tokenize(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14ec3b92-8525-42e5-8cd9-5d3b0f011e31",
   "metadata": {},
   "source": [
    "Далее почти во всех примерах и заданиях курса мы будем использовать именно _RegexpTokenizer_."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37f50633-0630-4bc0-9581-68c23f597b74",
   "metadata": {},
   "source": [
    "Теперь рассмотрим пример стемминга с использованием модулям _stem_.\n",
    "\n",
    "Будем делать это на примере следующего списка английских и русских слов:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8278b3c2-eefe-4d05-b430-57f689e30e2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['program', 'programs', 'programmer', 'programming', 'programmers', 'программа', 'программы', 'программист', 'программирование', 'программисты']\n"
     ]
    }
   ],
   "source": [
    "words = [\n",
    "            \"program\", \"programs\", \"programmer\", \"programming\", \"programmers\",           # en\n",
    "            \"программа\", \"программы\", \"программист\", \"программирование\", \"программисты\", # ru\n",
    "            ]\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64901faf-d9f9-47af-aeed-e874af8627ee",
   "metadata": {},
   "source": [
    "Простейший стеммер для английского языка это т.н. стеммер Портера: https://ru.wikipedia.org/wiki/%D0%A1%D1%82%D0%B5%D0%BC%D0%BC%D0%B5%D1%80_%D0%9F%D0%BE%D1%80%D1%82%D0%B5%D1%80%D0%B0\n",
    "\n",
    "Попробуем им воспользоваться:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ff9d88e1-afb9-43ff-9907-7d7c98b2b1c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEMMED (porter): 'program' -> 'program'\n",
      "STEMMED (porter): 'programs' -> 'program'\n",
      "STEMMED (porter): 'programmer' -> 'programm'\n",
      "STEMMED (porter): 'programming' -> 'program'\n",
      "STEMMED (porter): 'programmers' -> 'programm'\n",
      "STEMMED (porter): 'программа' -> 'программа'\n",
      "STEMMED (porter): 'программы' -> 'программы'\n",
      "STEMMED (porter): 'программист' -> 'программист'\n",
      "STEMMED (porter): 'программирование' -> 'программирование'\n",
      "STEMMED (porter): 'программисты' -> 'программисты'\n"
     ]
    }
   ],
   "source": [
    "stemmer = stem.PorterStemmer()\n",
    "for word in words:\n",
    "    term = stemmer.stem(word)\n",
    "    print(f\"STEMMED (porter): '{word}' -> '{term}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "230b873e-abe3-4d9d-864f-28b3968f52aa",
   "metadata": {},
   "source": [
    "Он поддерживает несколько языков:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3172ff18-9b40-4b91-8ad3-9c22c613b93b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEMMED (porter): 'program' -> 'program'\n",
      "STEMMED (porter): 'programs' -> 'program'\n",
      "STEMMED (porter): 'programmer' -> 'programm'\n",
      "STEMMED (porter): 'programming' -> 'program'\n",
      "STEMMED (porter): 'programmers' -> 'programm'\n",
      "STEMMED (porter): 'программа' -> 'программа'\n",
      "STEMMED (porter): 'программы' -> 'программы'\n",
      "STEMMED (porter): 'программист' -> 'программист'\n",
      "STEMMED (porter): 'программирование' -> 'программирование'\n",
      "STEMMED (porter): 'программисты' -> 'программисты'\n"
     ]
    }
   ],
   "source": [
    "stemmer = stem.PorterStemmer()\n",
    "for word in words:\n",
    "    term = stemmer.stem(word)\n",
    "    print(f\"STEMMED (porter): '{word}' -> '{term}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d6c2d55-0f0c-4d8d-bd1e-1e862552fd08",
   "metadata": {},
   "source": [
    "Мы видим, что стеммер хорошо отработал для английского языка, но, к сожалению, совсем не справился с русским.\n",
    "\n",
    "Для работы с не-англоязычными текстами обычно рекомендуют стеммер Snowball: https://snowballstem.org/\n",
    "\n",
    "Он поддерживает несколько языков:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a0a9c2d2-e2ad-4fcc-8c02-7cf3dc34b3fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('arabic', 'danish', 'dutch', 'english', 'finnish', 'french', 'german', 'hungarian', 'italian', 'norwegian', 'porter', 'portuguese', 'romanian', 'russian', 'spanish', 'swedish')\n"
     ]
    }
   ],
   "source": [
    "print(stem.SnowballStemmer.languages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df26f3a5-1e3e-4ae9-b6b3-14de3f517d7d",
   "metadata": {},
   "source": [
    "Попробуем воспользоваться русскоязычным вариантом:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0bc3c9d3-9fb3-40e4-adb3-2e6d34baa05f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEMMED (porter): 'program' -> 'program'\n",
      "STEMMED (porter): 'programs' -> 'programs'\n",
      "STEMMED (porter): 'programmer' -> 'programmer'\n",
      "STEMMED (porter): 'programming' -> 'programming'\n",
      "STEMMED (porter): 'programmers' -> 'programmers'\n",
      "STEMMED (porter): 'программа' -> 'программ'\n",
      "STEMMED (porter): 'программы' -> 'программ'\n",
      "STEMMED (porter): 'программист' -> 'программист'\n",
      "STEMMED (porter): 'программирование' -> 'программирован'\n",
      "STEMMED (porter): 'программисты' -> 'программист'\n"
     ]
    }
   ],
   "source": [
    "stemmer = stem.SnowballStemmer('russian')\n",
    "for word in words:\n",
    "    term = stemmer.stem(word)\n",
    "    print(f\"STEMMED (porter): '{word}' -> '{term}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c409e3ff-779b-4100-b30e-cf9d64e8e059",
   "metadata": {},
   "source": [
    "Видим, что теперь с русским языком все хорошо, но зато сломался стемминг английских слов -- чтобы его починить, придется воспользоваться создать объект _SnowballStemmer('english')_.\n",
    "\n",
    "К сожалению, это типичная ситуация, и на практике вам как правило надо сначала каким-то образом определить язык документов, с которым вы работаете, и только потом применить к ним соответствующий стеммер или лемматизатор."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2d8801e-39dd-4844-9240-9a71c3625e83",
   "metadata": {},
   "source": [
    "## Токенизация и лемматизация с использованием библиотеки spaCy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63ef0386-af81-4cd3-821f-b2a28d839171",
   "metadata": {},
   "source": [
    "Библиотека NLTK хорошо справляется со стеммингом но, если мы хотим пойти дальше и лемматизировать наши термы, нам придется воспользоваться библиотекой spaCy, т.к. в NLTK нет встроенного лемматизатора для русского языка (есть только для английского).\n",
    "\n",
    "Начнем с того что импортируем модули:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "70f8c76e-e7f9-43f3-9c0d-d9076d242a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "457e6d90-1111-40cd-a2ce-f2a6a2bc580e",
   "metadata": {},
   "source": [
    "И скачаем модель для русского языка:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "67fb4cd6-6a57-4f33-81c5-74bf492aef01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ru-core-news-md==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/ru_core_news_md-3.8.0/ru_core_news_md-3.8.0-py3-none-any.whl (41.9 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.9/41.9 MB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\u001b[36m0:00:01\u001b[0mm eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting pymorphy3>=1.0.0 (from ru-core-news-md==3.8.0)\n",
      "  Using cached pymorphy3-2.0.2-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting dawg-python>=0.7.1 (from pymorphy3>=1.0.0->ru-core-news-md==3.8.0)\n",
      "  Using cached DAWG_Python-0.7.2-py2.py3-none-any.whl.metadata (7.0 kB)\n",
      "Collecting pymorphy3-dicts-ru (from pymorphy3>=1.0.0->ru-core-news-md==3.8.0)\n",
      "  Using cached pymorphy3_dicts_ru-2.4.417150.4580142-py2.py3-none-any.whl.metadata (2.0 kB)\n",
      "Requirement already satisfied: setuptools>=68.2.2 in /home/andrei/edu/vk-ir-course-fall-2024/.venvs/seminar-05-text-ranking-p1/lib/python3.12/site-packages (from pymorphy3>=1.0.0->ru-core-news-md==3.8.0) (75.2.0)\n",
      "Using cached pymorphy3-2.0.2-py3-none-any.whl (53 kB)\n",
      "Using cached DAWG_Python-0.7.2-py2.py3-none-any.whl (11 kB)\n",
      "Using cached pymorphy3_dicts_ru-2.4.417150.4580142-py2.py3-none-any.whl (8.4 MB)\n",
      "Installing collected packages: pymorphy3-dicts-ru, dawg-python, pymorphy3, ru-core-news-md\n",
      "Successfully installed dawg-python-0.7.2 pymorphy3-2.0.2 pymorphy3-dicts-ru-2.4.417150.4580142 ru-core-news-md-3.8.0\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('ru_core_news_md')\n",
      "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
      "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
      "order to load all the package's dependencies. You can do this by selecting the\n",
      "'Restart kernel' or 'Restart runtime' option.\n"
     ]
    }
   ],
   "source": [
    "spacy.cli.download(\"ru_core_news_md\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ab54e72-f327-4808-b41c-fa05514bf3ae",
   "metadata": {},
   "source": [
    "Загрузим эту модель:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6002a6b2-a141-43b5-87cd-87138ef0b1c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<spacy.lang.ru.Russian object at 0x7f98cd39a270>\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load(\"ru_core_news_md\")\n",
    "print(nlp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3701a8ce-798a-4eb5-88fa-aa5495066240",
   "metadata": {},
   "source": [
    "И применим к нашим текстам:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7e57ed38-282a-4e35-95fe-13e53f5f0b90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc = 0 tokens = [George, Washington, lives, in, Washington] lemmas = ['george', 'washington', 'lives', 'in', 'washington']\n",
      "doc = 1 tokens = [Thomas, Jefferson, lives, in, New, York] lemmas = ['thomas', 'jefferson', 'lives', 'in', 'new', 'york']\n",
      "doc = 2 tokens = [Джордж, Вашингтон, живет, Вашингтоне] lemmas = ['джордж', 'вашингтон', 'жить', 'вашингтон']\n",
      "doc = 3 tokens = [Томас, Джефферсон, живет, Нью, Йорке] lemmas = ['томас', 'джефферсон', 'жить', 'нью', 'йорк']\n",
      "doc = 4 tokens = [The, U.S., are, a, country, The, U.N., is, an, organization] lemmas = ['the', 'u.s.', 'are', 'a', 'country', 'the', 'u.n.', 'is', 'an', 'organization']\n",
      "doc = 5 tokens = [It, 's, obvious] lemmas = ['it', \"'s\", 'obvious']\n"
     ]
    }
   ],
   "source": [
    "# Create spaCy document objects\n",
    "docs = []\n",
    "for text in texts:\n",
    "    doc = nlp(text)\n",
    "    docs.append(doc)\n",
    "\n",
    "# Iterate docs and show tokens and lemmas\n",
    "for i, doc in enumerate(docs):\n",
    "    tokens = []\n",
    "    lemmas = []\n",
    "    for token in doc:\n",
    "        # Skip stop words, punctuation, etc.\n",
    "        if token.is_stop or token.is_punct or token.is_space or token.is_digit:\n",
    "            continue\n",
    "        tokens.append(token)\n",
    "        lemmas.append(token.lemma_)\n",
    "\n",
    "    # Print doc tokens and lemmas\n",
    "    print(f\"doc = {i} tokens = {tokens} lemmas = {lemmas}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fae6d4dc-90c1-4d87-bf1d-65d31dd1824e",
   "metadata": {},
   "source": [
    "Мы видим, что для русского языка spaCy прекрасно справилось как с токенизацией, так и с лемматизацией!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ff6c1b0-56ad-4e77-ad1b-4e996839b24c",
   "metadata": {},
   "source": [
    "## Лемматизация с использованием библиотеки pymorphy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5c657bd-3789-4433-87ac-85245880d5d9",
   "metadata": {},
   "source": [
    "В тех случаях, когда нам нужно лемматизировать русскоязычный текст, но нет желания иметь в зависимостях такую большую и сложную библиотеку как spaCy, лучше всего подойдет библиотека pymorphy.\n",
    "\n",
    "Импортируем ее:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "942a4a47-3a85-4190-ad10-30d3b4c163f4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pymorphy3 as pymorphy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fdcf20e-17c7-452c-8ab5-e3e9c8ca91b8",
   "metadata": {},
   "source": [
    "Будем лемматизировать набор русскоязычных слов:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "af524e0d-a85a-4df9-95a3-b97745d0fd20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['программа', 'программы', 'программист', 'программирование', 'программисты', 'программистами', 'стали', 'люди', 'гулял', 'гранит', 'стекло', 'бутявковедами']\n"
     ]
    }
   ],
   "source": [
    "words = [\n",
    "            \"программа\", \"программы\", \"программист\", \"программирование\", \"программисты\", \"программистами\", # simple cases\n",
    "            \"стали\", \"люди\", \"гулял\", \"гранит\", \"стекло\", \"бутявковедами\"                                  # more complex\n",
    "            ]\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c522364-489c-487d-a696-32acd831f0ec",
   "metadata": {},
   "source": [
    "Создадим объект-анализатор и применим его к нашим словам:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ca180278-b8c1-428b-b3aa-555e68245ff7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "form #0: 'программа' -> 'программа' (score: 1.000)\n",
      "form #0: 'программы' -> 'программа' (score: 0.619)\n",
      "form #1: 'программы' -> 'программа' (score: 0.233)\n",
      "form #2: 'программы' -> 'программа' (score: 0.148)\n",
      "form #0: 'программист' -> 'программист' (score: 1.000)\n",
      "form #0: 'программирование' -> 'программирование' (score: 0.750)\n",
      "form #1: 'программирование' -> 'программирование' (score: 0.250)\n",
      "form #0: 'программисты' -> 'программист' (score: 1.000)\n",
      "form #0: 'программистами' -> 'программист' (score: 1.000)\n",
      "form #0: 'стали' -> 'стать' (score: 0.975)\n",
      "form #1: 'стали' -> 'сталь' (score: 0.011)\n",
      "form #2: 'стали' -> 'сталь' (score: 0.005)\n",
      "form #3: 'стали' -> 'сталь' (score: 0.003)\n",
      "form #4: 'стали' -> 'сталь' (score: 0.003)\n",
      "form #5: 'стали' -> 'сталь' (score: 0.003)\n",
      "form #0: 'люди' -> 'человек' (score: 1.000)\n",
      "form #0: 'гулял' -> 'гулять' (score: 1.000)\n",
      "form #0: 'гранит' -> 'гранит' (score: 0.200)\n",
      "form #1: 'гранит' -> 'гранит' (score: 0.200)\n",
      "form #2: 'гранит' -> 'гранита' (score: 0.200)\n",
      "form #3: 'гранит' -> 'гранита' (score: 0.200)\n",
      "form #4: 'гранит' -> 'гранить' (score: 0.200)\n",
      "form #0: 'стекло' -> 'стекло' (score: 0.690)\n",
      "form #1: 'стекло' -> 'стекло' (score: 0.286)\n",
      "form #2: 'стекло' -> 'стечь' (score: 0.024)\n",
      "form #0: 'бутявковедами' -> 'бутявковед' (score: 0.982)\n",
      "form #1: 'бутявковедами' -> 'бутявковёд' (score: 0.009)\n",
      "form #2: 'бутявковедами' -> 'бутявковёд' (score: 0.009)\n"
     ]
    }
   ],
   "source": [
    "morph = pymorphy.MorphAnalyzer(lang='ru')\n",
    "\n",
    "for word in words:\n",
    "    parsed = morph.parse(word)\n",
    "    for i, form in enumerate(parsed):\n",
    "        print(f\"form #{i}: '{word}' -> '{form.normal_form}' (score: {form.score:.3f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "865ee88d-f6bb-4338-a397-d138ede3b639",
   "metadata": {},
   "source": [
    "Обратим внимание, что для каждого слова pymorphy выдает несколько гипотез, упорядоченных по некоторому скору, который оценивает насколько вероятна та или иная гипотеза.\n",
    "\n",
    "Также, _pymorphy_ прекрасно справляется со сложными случаями, такими как ЛЮДИ, СТАЛЬ, СТЕКЛО, ГРАНИТ и т.п."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
