{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c68b6411-668d-431f-9556-9a478597aa6c",
   "metadata": {},
   "source": [
    "# Предобработка текста\n",
    "\n",
    "В этом тюториале мы научимся решать задачи токенизации, стемминга и лемматизации текста.\n",
    "\n",
    "Будем это делать с помощью трех популярных библиотек: _NLTK_, _spaCy_ и _pymorphy_.\n",
    "\n",
    "Ссылки на документацию:\n",
    "- _NLTK_: https://www.nltk.org/\n",
    "- _spaCy_: https://spacy.io/\n",
    "- _pymorphy_: https://github.com/no-plagiarism/pymorphy3 (см. также https://github.com/pymorphy2/pymorphy2 -- этот проект был заброшен автором и форкнут авторами _pymorphy3_)\n",
    "\n",
    "Не будем подробно останавливаться на отличиях _NLTK_ от _spaCy_, заметим только что в большинстве других примеров в данном курсе мы используем библиотеку _NLTK_."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fc459ca-2cd0-49bb-9670-461c981bffa9",
   "metadata": {},
   "source": [
    "## Токенизация и стемминг с использованием библиотеки NLTK"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f0a7027-7d31-480c-9e05-44b6ca966f68",
   "metadata": {},
   "source": [
    "Импортируем модули которые нам понадобятся впоследствии:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "21fb783d-937e-40a6-88b5-3df49b614a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import tokenize\n",
    "from nltk import stem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a26b075-1952-458c-b07c-d231b2202d90",
   "metadata": {},
   "source": [
    "Библиотека _NLTK_ будет использовать несколько дополнительных моделей, загрузим их:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ab81a53d-3512-4e4b-8583-15c72fe6f876",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/andrei/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to /home/andrei/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61bd5f69-0f93-4a21-8aa6-62744c5e6476",
   "metadata": {},
   "source": [
    "Рассмотрим маленькую коллекцию из нескольких текстовых документов:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d84b812b-f9bd-41dd-96dc-3007b4efe5e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['George Washington lives in Washington.',\n",
       " 'Thomas Jefferson lives in New York.',\n",
       " 'Джордж Вашингтон живет в Вашингтоне.',\n",
       " 'Томас Джефферсон живет в Нью-Йорке.',\n",
       " 'The U.S. are a country. The U.N. is an organization.',\n",
       " \"It's obvious!\"]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts = [\n",
    "    \"George Washington lives in Washington.\",\n",
    "    \"Thomas Jefferson lives in New York.\",\n",
    "    \"Джордж Вашингтон живет в Вашингтоне.\",\n",
    "    \"Томас Джефферсон живет в Нью-Йорке.\",\n",
    "    \"The U.S. are a country. The U.N. is an organization.\",\n",
    "    \"It's obvious!\",\n",
    "]\n",
    "texts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d8eed21-bb25-436c-88f3-4c8008c553fc",
   "metadata": {},
   "source": [
    "Попробуем их токенизировать.\n",
    "\n",
    "Простейший способ это сделать -- воспользоваться функцией _word_tokenize_:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "caf22b01-c63a-4bc4-936f-837471d14d53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['George', 'Washington', 'lives', 'in', 'Washington', '.']\n",
      "['Thomas', 'Jefferson', 'lives', 'in', 'New', 'York', '.']\n",
      "['Джордж', 'Вашингтон', 'живет', 'в', 'Вашингтоне', '.']\n",
      "['Томас', 'Джефферсон', 'живет', 'в', 'Нью-Йорке', '.']\n",
      "['The', 'U.S.', 'are', 'a', 'country', '.', 'The', 'U.N.', 'is', 'an', 'organization', '.']\n",
      "['It', \"'s\", 'obvious', '!']\n"
     ]
    }
   ],
   "source": [
    "for text in texts:\n",
    "    print(tokenize.word_tokenize(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e781ffb-e70a-4db2-b4c2-a450248fd6ac",
   "metadata": {},
   "source": [
    "Мы видим, что _word_tokenize_ позволяет делать умную токенизацию, которая, например, понимает что U.S. -- это страна, которая должна быть представлены в виде одного токена.\n",
    "\n",
    "На практике, нам, как правило, нужна не \"умная\" токенизация, а, скорее наоборот, что-то простое, и над чем мы имеем максимальный контроль. Для этого хорошо подходит класс _RegexpTokenizer_.\n",
    "\n",
    "С его помощью можно, например, просто посплитить по пробелам и пунктуации:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0a39ec59-b88c-4ebe-87bd-2b8da51b1746",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['George', 'Washington', 'lives', 'in', 'Washington']\n",
      "['Thomas', 'Jefferson', 'lives', 'in', 'New', 'York']\n",
      "['Джордж', 'Вашингтон', 'живет', 'в', 'Вашингтоне']\n",
      "['Томас', 'Джефферсон', 'живет', 'в', 'Нью', 'Йорке']\n",
      "['The', 'U', 'S', 'are', 'a', 'country', 'The', 'U', 'N', 'is', 'an', 'organization']\n",
      "['It', 's', 'obvious']\n"
     ]
    }
   ],
   "source": [
    "tokenizer = tokenize.RegexpTokenizer(r'\\w+')\n",
    "for text in texts:\n",
    "    print(tokenizer.tokenize(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2678b98a-4202-49e8-ba67-ef571798cbdc",
   "metadata": {},
   "source": [
    "Чуть более сложный паттерн, который позволяет оставить только слова длиной 2 или более символов:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "00d11d18-5690-41d9-ab7b-3ef99fce9a20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['George', 'Washington', 'lives', 'in', 'Washington']\n",
      "['Thomas', 'Jefferson', 'lives', 'in', 'New', 'York']\n",
      "['Джордж', 'Вашингтон', 'живет', 'Вашингтоне']\n",
      "['Томас', 'Джефферсон', 'живет', 'Нью', 'Йорке']\n",
      "['The', 'are', 'country', 'The', 'is', 'an', 'organization']\n",
      "['It', 'obvious']\n"
     ]
    }
   ],
   "source": [
    "tokenizer = tokenize.RegexpTokenizer(r'\\w\\w+')\n",
    "for text in texts:\n",
    "    print(tokenizer.tokenize(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14ec3b92-8525-42e5-8cd9-5d3b0f011e31",
   "metadata": {},
   "source": [
    "Далее почти во всех примерах и заданиях курса мы будем использовать именно _RegexpTokenizer_."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37f50633-0630-4bc0-9581-68c23f597b74",
   "metadata": {},
   "source": [
    "Теперь рассмотрим пример стемминга с использованием модуля _stem_ библиотеки _nltk_.\n",
    "\n",
    "Будем делать это на примере следующего списка английских и русских слов:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8278b3c2-eefe-4d05-b430-57f689e30e2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['program',\n",
       " 'programs',\n",
       " 'programmer',\n",
       " 'programming',\n",
       " 'programmers',\n",
       " 'программа',\n",
       " 'программы',\n",
       " 'программист',\n",
       " 'программирование',\n",
       " 'программисты']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = [\n",
    "            \"program\", \"programs\", \"programmer\", \"programming\", \"programmers\",           # en\n",
    "            \"программа\", \"программы\", \"программист\", \"программирование\", \"программисты\", # ru\n",
    "            ]\n",
    "words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64901faf-d9f9-47af-aeed-e874af8627ee",
   "metadata": {},
   "source": [
    "Простейший стеммер для английского языка это т.н. стеммер Портера: https://ru.wikipedia.org/wiki/%D0%A1%D1%82%D0%B5%D0%BC%D0%BC%D0%B5%D1%80_%D0%9F%D0%BE%D1%80%D1%82%D0%B5%D1%80%D0%B0\n",
    "\n",
    "Попробуем им воспользоваться:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ff9d88e1-afb9-43ff-9907-7d7c98b2b1c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEMMED (porter): 'program' -> 'program'\n",
      "STEMMED (porter): 'programs' -> 'program'\n",
      "STEMMED (porter): 'programmer' -> 'programm'\n",
      "STEMMED (porter): 'programming' -> 'program'\n",
      "STEMMED (porter): 'programmers' -> 'programm'\n",
      "STEMMED (porter): 'программа' -> 'программа'\n",
      "STEMMED (porter): 'программы' -> 'программы'\n",
      "STEMMED (porter): 'программист' -> 'программист'\n",
      "STEMMED (porter): 'программирование' -> 'программирование'\n",
      "STEMMED (porter): 'программисты' -> 'программисты'\n"
     ]
    }
   ],
   "source": [
    "stemmer = stem.PorterStemmer()\n",
    "for word in words:\n",
    "    term = stemmer.stem(word)\n",
    "    print(f\"STEMMED (porter): '{word}' -> '{term}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d6c2d55-0f0c-4d8d-bd1e-1e862552fd08",
   "metadata": {},
   "source": [
    "Мы видим, что стеммер хорошо отработал для английского языка, но, к сожалению, совсем не справился с русским.\n",
    "\n",
    "Для работы с не-англоязычными текстами обычно рекомендуют стеммер Snowball: https://snowballstem.org/\n",
    "\n",
    "Он поддерживает несколько языков, в т.ч. русский:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a0a9c2d2-e2ad-4fcc-8c02-7cf3dc34b3fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('arabic',\n",
       " 'danish',\n",
       " 'dutch',\n",
       " 'english',\n",
       " 'finnish',\n",
       " 'french',\n",
       " 'german',\n",
       " 'hungarian',\n",
       " 'italian',\n",
       " 'norwegian',\n",
       " 'porter',\n",
       " 'portuguese',\n",
       " 'romanian',\n",
       " 'russian',\n",
       " 'spanish',\n",
       " 'swedish')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stem.SnowballStemmer.languages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df26f3a5-1e3e-4ae9-b6b3-14de3f517d7d",
   "metadata": {},
   "source": [
    "Попробуем воспользоваться русскоязычным вариантом:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0bc3c9d3-9fb3-40e4-adb3-2e6d34baa05f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEMMED (snowball/russian): 'program' -> 'program'\n",
      "STEMMED (snowball/russian): 'programs' -> 'programs'\n",
      "STEMMED (snowball/russian): 'programmer' -> 'programmer'\n",
      "STEMMED (snowball/russian): 'programming' -> 'programming'\n",
      "STEMMED (snowball/russian): 'programmers' -> 'programmers'\n",
      "STEMMED (snowball/russian): 'программа' -> 'программ'\n",
      "STEMMED (snowball/russian): 'программы' -> 'программ'\n",
      "STEMMED (snowball/russian): 'программист' -> 'программист'\n",
      "STEMMED (snowball/russian): 'программирование' -> 'программирован'\n",
      "STEMMED (snowball/russian): 'программисты' -> 'программист'\n"
     ]
    }
   ],
   "source": [
    "stemmer = stem.SnowballStemmer('russian')\n",
    "for word in words:\n",
    "    term = stemmer.stem(word)\n",
    "    print(f\"STEMMED (snowball/russian): '{word}' -> '{term}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c409e3ff-779b-4100-b30e-cf9d64e8e059",
   "metadata": {},
   "source": [
    "Видим, что теперь с русским языком все хорошо, но зато сломался стемминг английских слов -- чтобы его починить, придется воспользоваться создать объект _SnowballStemmer('english')_.\n",
    "\n",
    "К сожалению, это типичная ситуация, и на практике вам как правило надо сначала каким-то образом определить язык документа, с которым вы работаете, и только потом применить к нему соответствующий стеммер или лемматизатор."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2d8801e-39dd-4844-9240-9a71c3625e83",
   "metadata": {},
   "source": [
    "## Токенизация и лемматизация с использованием библиотеки spaCy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63ef0386-af81-4cd3-821f-b2a28d839171",
   "metadata": {},
   "source": [
    "Библиотека _NLTK_ хорошо справляется со стеммингом но, если мы хотим пойти дальше и лемматизировать наши термины, нам придется воспользоваться библиотекой _spaCy_, т.к. в _NLTK_ нет встроенного лемматизатора для русского языка (есть только для английского).\n",
    "\n",
    "Начнем с того что импортируем модули:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "70f8c76e-e7f9-43f3-9c0d-d9076d242a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "457e6d90-1111-40cd-a2ce-f2a6a2bc580e",
   "metadata": {},
   "source": [
    "И скачаем модель для русского языка:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "67fb4cd6-6a57-4f33-81c5-74bf492aef01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spaCy model ru_core_news_md is already downloaded\n"
     ]
    }
   ],
   "source": [
    "model_name = \"ru_core_news_md\"\n",
    "\n",
    "# Is already downloaded?\n",
    "if not spacy.util.is_package(model_name):\n",
    "    spacy.cli.download(model_name)\n",
    "else:\n",
    "    print(f\"spaCy model {model_name} is already downloaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ab54e72-f327-4808-b41c-fa05514bf3ae",
   "metadata": {},
   "source": [
    "Загрузим эту модель:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6002a6b2-a141-43b5-87cd-87138ef0b1c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<spacy.lang.ru.Russian object at 0x7fb8f22d65a0>\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load(\"ru_core_news_md\")\n",
    "print(nlp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3701a8ce-798a-4eb5-88fa-aa5495066240",
   "metadata": {},
   "source": [
    "И применим к нашим текстам:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7e57ed38-282a-4e35-95fe-13e53f5f0b90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc = 0 tokens = [George, Washington, lives, in, Washington] lemmas = ['george', 'washington', 'lives', 'in', 'washington']\n",
      "doc = 1 tokens = [Thomas, Jefferson, lives, in, New, York] lemmas = ['thomas', 'jefferson', 'lives', 'in', 'new', 'york']\n",
      "doc = 2 tokens = [Джордж, Вашингтон, живет, Вашингтоне] lemmas = ['джордж', 'вашингтон', 'жить', 'вашингтон']\n",
      "doc = 3 tokens = [Томас, Джефферсон, живет, Нью, Йорке] lemmas = ['томас', 'джефферсон', 'жить', 'нью', 'йорк']\n",
      "doc = 4 tokens = [The, U.S., are, a, country, The, U.N., is, an, organization] lemmas = ['the', 'u.s.', 'are', 'a', 'country', 'the', 'u.n.', 'is', 'an', 'organization']\n",
      "doc = 5 tokens = [It, 's, obvious] lemmas = ['it', \"'s\", 'obvious']\n"
     ]
    }
   ],
   "source": [
    "# Iterate all text\n",
    "for i, text in enumerate(texts):\n",
    "    # Create spaCy document object\n",
    "    doc = nlp(text)\n",
    "\n",
    "    # Collect document tokens and lemmas\n",
    "    tokens = []\n",
    "    lemmas = []\n",
    "    for token in doc:\n",
    "        # Skip stop words, punctuation, etc.\n",
    "        if token.is_stop or token.is_punct or token.is_space or token.is_digit:\n",
    "            continue\n",
    "        tokens.append(token)\n",
    "        lemmas.append(token.lemma_)\n",
    "\n",
    "    # Print doc tokens and lemmas\n",
    "    print(f\"doc = {i} tokens = {tokens} lemmas = {lemmas}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fae6d4dc-90c1-4d87-bf1d-65d31dd1824e",
   "metadata": {},
   "source": [
    "Мы видим, что для русского языка _spaCy_ прекрасно справилось как с токенизацией, так и с лемматизацией!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ff6c1b0-56ad-4e77-ad1b-4e996839b24c",
   "metadata": {},
   "source": [
    "## Лемматизация с использованием библиотеки pymorphy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5c657bd-3789-4433-87ac-85245880d5d9",
   "metadata": {},
   "source": [
    "В тех случаях, когда нам нужно лемматизировать русскоязычный текст, но нет желания иметь в зависимостях такую большую и сложную библиотеку как _spaCy_, лучше всего подойдет библиотека _pymorphy_.\n",
    "\n",
    "Импортируем ее:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "942a4a47-3a85-4190-ad10-30d3b4c163f4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pymorphy3 as pymorphy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6047a71f-fe0f-486b-a2c3-93d8f089692d",
   "metadata": {},
   "source": [
    "Обратите внимание, что мы тут используем именно _pymorphy3_ -- это новая реинкарнация библиотеки _pymorphy_, которая пришла на смену заброшенной авторами _pymorphy2_."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fdcf20e-17c7-452c-8ab5-e3e9c8ca91b8",
   "metadata": {},
   "source": [
    "Будем лемматизировать набор русскоязычных слов:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "af524e0d-a85a-4df9-95a3-b97745d0fd20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['программа',\n",
       " 'программы',\n",
       " 'программист',\n",
       " 'программирование',\n",
       " 'программисты',\n",
       " 'программистами',\n",
       " 'стали',\n",
       " 'люди',\n",
       " 'гулял',\n",
       " 'гранит',\n",
       " 'стекло',\n",
       " 'бутявковедами']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = [\n",
    "            \"программа\", \"программы\", \"программист\", \"программирование\", \"программисты\", \"программистами\", # simple cases\n",
    "            \"стали\", \"люди\", \"гулял\", \"гранит\", \"стекло\", \"бутявковедами\"                                  # more complex\n",
    "            ]\n",
    "words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c522364-489c-487d-a696-32acd831f0ec",
   "metadata": {},
   "source": [
    "Создадим объект-анализатор и применим его к нашим словам:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ca180278-b8c1-428b-b3aa-555e68245ff7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "form #0: 'программа' -> 'программа' (score: 1.000)\n",
      "form #0: 'программы' -> 'программа' (score: 0.619)\n",
      "form #1: 'программы' -> 'программа' (score: 0.233)\n",
      "form #2: 'программы' -> 'программа' (score: 0.148)\n",
      "form #0: 'программист' -> 'программист' (score: 1.000)\n",
      "form #0: 'программирование' -> 'программирование' (score: 0.750)\n",
      "form #1: 'программирование' -> 'программирование' (score: 0.250)\n",
      "form #0: 'программисты' -> 'программист' (score: 1.000)\n",
      "form #0: 'программистами' -> 'программист' (score: 1.000)\n",
      "form #0: 'стали' -> 'стать' (score: 0.975)\n",
      "form #1: 'стали' -> 'сталь' (score: 0.011)\n",
      "form #2: 'стали' -> 'сталь' (score: 0.005)\n",
      "form #3: 'стали' -> 'сталь' (score: 0.003)\n",
      "form #4: 'стали' -> 'сталь' (score: 0.003)\n",
      "form #5: 'стали' -> 'сталь' (score: 0.003)\n",
      "form #0: 'люди' -> 'человек' (score: 1.000)\n",
      "form #0: 'гулял' -> 'гулять' (score: 1.000)\n",
      "form #0: 'гранит' -> 'гранит' (score: 0.200)\n",
      "form #1: 'гранит' -> 'гранит' (score: 0.200)\n",
      "form #2: 'гранит' -> 'гранита' (score: 0.200)\n",
      "form #3: 'гранит' -> 'гранита' (score: 0.200)\n",
      "form #4: 'гранит' -> 'гранить' (score: 0.200)\n",
      "form #0: 'стекло' -> 'стекло' (score: 0.690)\n",
      "form #1: 'стекло' -> 'стекло' (score: 0.286)\n",
      "form #2: 'стекло' -> 'стечь' (score: 0.024)\n",
      "form #0: 'бутявковедами' -> 'бутявковед' (score: 0.982)\n",
      "form #1: 'бутявковедами' -> 'бутявковёд' (score: 0.009)\n",
      "form #2: 'бутявковедами' -> 'бутявковёд' (score: 0.009)\n"
     ]
    }
   ],
   "source": [
    "morph = pymorphy.MorphAnalyzer(lang='ru')\n",
    "\n",
    "for word in words:\n",
    "    parsed = morph.parse(word)\n",
    "    for i, form in enumerate(parsed):\n",
    "        print(f\"form #{i}: '{word}' -> '{form.normal_form}' (score: {form.score:.3f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "865ee88d-f6bb-4338-a397-d138ede3b639",
   "metadata": {},
   "source": [
    "Обратим внимание, что для каждого слова _pymorphy_ выдает несколько гипотез, упорядоченных по некоторому скору, который оценивает насколько вероятна та или иная гипотеза.\n",
    "\n",
    "Также, видно что _pymorphy_ прекрасно справляется со сложными случаями, такими как ЛЮДИ, СТАЛЬ, СТЕКЛО, ГРАНИТ и т.п."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
