{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c68b6411-668d-431f-9556-9a478597aa6c",
   "metadata": {},
   "source": [
    "# Латентно-семантический анализ (Latent Semantic Analysis или LSA)\n",
    "\n",
    "В этом тюториале мы:\n",
    "- познакомимся со стандартным датасетом _20 Newsgroups_\n",
    "- научимся использовать метод латентно-семантическго анализа (LSA) для представления текстов в виде плотных (dense) эмбеддингов, компонентами которых являются скрытые признаки-тематики\n",
    "- научимся использовать LSA-эмбеддинги для решения задач семантической близости и текстового ранжирования\n",
    "\n",
    "При подготовке этого тюториала использовались материалы из (отличной) книги _Natural Language Processing in Action_ (авторы _Hobson Lane_ и _Maria Dyshel_): https://www.goodreads.com/book/show/59694556-natural-language-processing-in-action-second-edition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f0a7027-7d31-480c-9e05-44b6ca966f68",
   "metadata": {},
   "source": [
    "Импортируем модули которые нам понадобятся впоследствии:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "21fb783d-937e-40a6-88b5-3df49b614a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "from timeit import default_timer as timer\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import pairwise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23d43408-4e27-4eed-9cc2-c0839c707f92",
   "metadata": {},
   "source": [
    "## Датасет 20 Newgroups"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63ef0386-af81-4cd3-821f-b2a28d839171",
   "metadata": {},
   "source": [
    "Наш тюториал будет основан на классическом датасете 20 Newsgroups.\n",
    "\n",
    "Этот датасет был собран еще в начале 90-х годов, и представляет из себя коллекцию из примерно 20000 текстовых сообщений, отправленных в 20 разных каналах (newgroups) в сети Usenet (https://en.wikipedia.org/wiki/Usenet_newsgroup). Название канала (напр. _sci.space_, т.е. канал про космос) используется как метка класса.\n",
    "\n",
    "Другими словами, это небольшой текстовый датасет для мультиклассовой классификации, всего 20 классов (по числу каналов).\n",
    "\n",
    "Этот датасет приобрел очень большую популярность в мире NLP, и широко используется для иллюстрации работы различных алгоритмов, моделей и т.п.\n",
    "\n",
    "Кроме того, этим датасетом очень удобно пользоваться, т.к. он доступен \"из коробки\" в библиотеке _scikit-learn_: https://scikit-learn.org/stable/datasets/real_world.html#the-20-newsgroups-text-dataset\n",
    "\n",
    "Попробуем загрузить датасет:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "70f8c76e-e7f9-43f3-9c0d-d9076d242a12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fetched 20 newsgroups dataset: num_docs = 11314\n"
     ]
    }
   ],
   "source": [
    "# Filter e-mail headers, signature blocks and quotes\n",
    "data_remove = ('headers', 'footers', 'quotes')\n",
    "\n",
    "# Fetcher options\n",
    "data_kwargs = { 'remove': data_remove, 'shuffle': True, 'random_state': 22 }\n",
    "\n",
    "# Fetch train subset. \n",
    "# Bunch contains:\n",
    "#   - data          -- list of text documents\n",
    "#   - filenames     -- list of sample filenames\n",
    "#   - target        -- numeric target in [0, 19]\n",
    "#   - target_names  -- list of target names\n",
    "train_bunch = datasets.fetch_20newsgroups(subset=\"train\", **data_kwargs)\n",
    "print(f\"fetched 20 newsgroups dataset: num_docs = {len(train_bunch.data)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "457e6d90-1111-40cd-a2ce-f2a6a2bc580e",
   "metadata": {},
   "source": [
    "Обратите внимание, что:\n",
    "- датасет поделен на train и test части, нас дальше будет интересовать только train\n",
    "- мы скачиваем очищенный вариант датасета с удаленной из сообщений служебной информацией, такой как заголовки писем и т.п.\n",
    "\n",
    "Всего у нас скачалось ~11 тыс. документов.\n",
    "\n",
    "Посмотрим теперь на метки классов:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1d3494f5-16e7-49cd-9c71-ca3f85cecaae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10 16 11 ...  9 15  6]\n",
      "['alt.atheism', 'comp.graphics', 'comp.os.ms-windows.misc', 'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware', 'comp.windows.x', 'misc.forsale', 'rec.autos', 'rec.motorcycles', 'rec.sport.baseball', 'rec.sport.hockey', 'sci.crypt', 'sci.electronics', 'sci.med', 'sci.space', 'soc.religion.christian', 'talk.politics.guns', 'talk.politics.mideast', 'talk.politics.misc', 'talk.religion.misc']\n"
     ]
    }
   ],
   "source": [
    "print(train_bunch.target)\n",
    "print(train_bunch.target_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fed1494-3cf6-4452-bc8a-c36cd025fc72",
   "metadata": {},
   "source": [
    "Видно, что таргеты доступны как в виде текстов (названий каналов), так и в виде числовых идентификаторов.\n",
    "\n",
    "Посмотрим теперь на типичные тексты сообщений:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0e5a0d16-67f5-4653-8dc6-670210a2b929",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "If the Islanders beat the Devils tonight, they would finish with\n",
      "identical records.  Who's the lucky team that gets to face the Penguins\n",
      "in the opening round?   Also, can somebody list the rules for breaking\n",
      "ties.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "docs = train_bunch.data\n",
    "print(docs[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07def150-f9b2-4d2d-8bb8-01b526dc08b2",
   "metadata": {},
   "source": [
    "К какому классу относится наше сообщение?\n",
    "\n",
    "Это легко понять следующим образом:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "577b554b-0129-4ca5-86c7-58e330b41a0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rec.sport.hockey\n"
     ]
    }
   ],
   "source": [
    "y_0 = train_bunch.target[0]\n",
    "print(train_bunch.target_names[y_0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f27cfed-e8b4-46c8-9897-a54c6d79f794",
   "metadata": {},
   "source": [
    "Как (наверное) нетрудно было догадаться, это сообщение про хоккей, отправленное в канал _rec.sport.hockey_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aee774c-e382-43e4-9b26-16e9190c309d",
   "metadata": {},
   "source": [
    "## LSA-эмбеддинги"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c5e1518-ac81-456e-97f3-9bffee59ffa3",
   "metadata": {},
   "source": [
    "Попробуем теперь воспользоваться методом LSA и получить эмбеддинги наших текстов.\n",
    "\n",
    "Первым делом надо превратить текст в матрицу термин-документ, т.е. получить для него разреженное признаковое представление.<br>\n",
    "Сразу обратим внимание, что в качестве весов в ячейках матрицы термин-документ можно бы было использовать любой из методов взвешивания, изученных нами в лекции про модели векторого пространства, например:\n",
    "- бинарные веса 1 (слово есть в документе) или 0 (слова нет в документе), которые проще всего получить с помощью класса _CountVectorizer_ из библиотеки _scikit-learn_\n",
    "- TFы, т.е. частоты слова в документах\n",
    "- TF-IDFы с разными способами нормализации, сглаживания и т.п.\n",
    "\n",
    "На практике, как правило, в качестве признаков лучше всего работают TF-IDFы, поэтому мы дальше будем использовать именно их.<br>\n",
    "Векторизуем наши тексты с помощью класса _TfidfVectorizer_:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "67fb4cd6-6a57-4f33-81c5-74bf492aef01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'scipy.sparse._csr.csr_matrix'>\n",
      "vectorized: X_docs.shape = (11314, 5358) elapsed = 0.769\n"
     ]
    }
   ],
   "source": [
    "# Prepare vectorizer\n",
    "vectorizer = TfidfVectorizer(min_df=0.002, max_df=0.5, stop_words='english', decode_error='ignore')\n",
    "\n",
    "# Vectorize\n",
    "start = timer()\n",
    "X_docs = vectorizer.fit_transform(docs)\n",
    "print(type(X_docs))\n",
    "print(f\"vectorized: X_docs.shape = {X_docs.shape} elapsed = {timer() - start:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ab54e72-f327-4808-b41c-fa05514bf3ae",
   "metadata": {},
   "source": [
    "При векторизации мы сразу выкидываем стоп-слова (параметр _stop_words_), а также слишком редкие (параметр _min_df_) и слишком часто встречающиеся слова (_max_df_).\n",
    "\n",
    "В результате у нас получилась матрица термин-документ размером 11314x5358, т.е. 11314 документов и 5358 терминов (слов).\n",
    "\n",
    "Обратим внимание, что при желании мы могли бы использовать тут стемминг, лемматизацию или какую-то кастомную токенизацию, но не делаем этого чтобы не загромождать тюториал.\n",
    "\n",
    "Наша матрица содержит TF-IDF весах, причем все векторы документы нормализованы с использованием L2-нормы (это дефолтное поведение векторизатора):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "644721a6-82ff-4a06-89ab-db2a58f3b249",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "print(X_docs.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "697d0810-0356-47d3-84ff-91823c0700d6",
   "metadata": {},
   "source": [
    "Видно, что матрица крайне разреженная, и действительно:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6002a6b2-a141-43b5-87cd-87138ef0b1c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "516701\n"
     ]
    }
   ],
   "source": [
    "print(X_docs.nnz)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3701a8ce-798a-4eb5-88fa-aa5495066240",
   "metadata": {},
   "source": [
    "Т.е. у нас заполнено только 516701 / (11314 * 5358) = 0.0085 элементов!\n",
    "\n",
    "Посмотрим сразу и на термины, которые использует векторизатор:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7e57ed38-282a-4e35-95fe-13e53f5f0b90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>00</th>\n",
       "      <th>000</th>\n",
       "      <th>01</th>\n",
       "      <th>02</th>\n",
       "      <th>03</th>\n",
       "      <th>04</th>\n",
       "      <th>05</th>\n",
       "      <th>06</th>\n",
       "      <th>07</th>\n",
       "      <th>08</th>\n",
       "      <th>...</th>\n",
       "      <th>yesterday</th>\n",
       "      <th>york</th>\n",
       "      <th>young</th>\n",
       "      <th>younger</th>\n",
       "      <th>youth</th>\n",
       "      <th>yup</th>\n",
       "      <th>zero</th>\n",
       "      <th>zip</th>\n",
       "      <th>zone</th>\n",
       "      <th>zoom</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 5358 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    00  000   01   02   03   04   05   06   07   08  ...  yesterday  york  \\\n",
       "0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...        0.0   0.0   \n",
       "1  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...        0.0   0.0   \n",
       "2  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...        0.0   0.0   \n",
       "3  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...        0.0   0.0   \n",
       "4  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...        0.0   0.0   \n",
       "\n",
       "   young  younger  youth  yup  zero  zip  zone  zoom  \n",
       "0    0.0      0.0    0.0  0.0   0.0  0.0   0.0   0.0  \n",
       "1    0.0      0.0    0.0  0.0   0.0  0.0   0.0   0.0  \n",
       "2    0.0      0.0    0.0  0.0   0.0  0.0   0.0   0.0  \n",
       "3    0.0      0.0    0.0  0.0   0.0  0.0   0.0   0.0  \n",
       "4    0.0      0.0    0.0  0.0   0.0  0.0   0.0   0.0  \n",
       "\n",
       "[5 rows x 5358 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = vectorizer.get_feature_names_out()\n",
    "docs_df = pd.DataFrame(X_docs.toarray(), columns=vocab)\n",
    "docs_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fae6d4dc-90c1-4d87-bf1d-65d31dd1824e",
   "metadata": {},
   "source": [
    "Теперь начинается самое интересное.\n",
    "\n",
    "Применим к нашей матрице SVD-разложение:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3b6d9cf8-0699-4ce0-ac11-8c4fd756c165",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.043  0.015 -0.051 ... -0.002  0.005  0.013]\n",
      " [ 0.015  0.008 -0.    ...  0.012 -0.001  0.002]\n",
      " [ 0.098 -0.011 -0.029 ... -0.005 -0.035 -0.045]\n",
      " ...\n",
      " [ 0.084  0.002  0.025 ...  0.005  0.011  0.1  ]\n",
      " [ 0.159  0.157  0.18  ... -0.021 -0.013  0.004]\n",
      " [ 0.     0.     0.    ...  0.     0.     0.   ]]\n",
      "decomposed: E_docs.shape = (11314, 20) elapsed = 4.253\n"
     ]
    }
   ],
   "source": [
    "# Prepare SVD\n",
    "svd = TruncatedSVD(n_components=20, n_iter=100, random_state=22)\n",
    "\n",
    "# Run SVD\n",
    "start = timer()\n",
    "E_docs = svd.fit_transform(X_docs)\n",
    "print(E_docs.round(3))\n",
    "print(f\"decomposed: E_docs.shape = {E_docs.shape} elapsed = {timer() - start:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afee7523-8a85-4d2d-8f1e-5693b00637fb",
   "metadata": {},
   "source": [
    "Мы используем класс _TruncatedSVD_ из библиотеки _scikit-learn_, а в качестве параметра _n_components_ передаем ему желаемый размер нашего скрытого пространства.\n",
    "\n",
    "Размер скрытого пространства является, по сути, гиперпараметром нашего метода и его можно попробовать подобрать под целевую задачу, в которой мы впоследствии будем использовать наши эмбеддинги. На практике, как правило, хорошо работают эмбеддинги размером 100-300 элементов.\n",
    "\n",
    "В данном примере мы будем использовать _n_components_ равное 20, потому что:\n",
    "- нам заранее известно, что у нас всего 20 классов, т.е. кажется что 20 -- это минимальное количество тематик, которыми можно описать нашу коллекцию\n",
    "- с другой стороны, хотим показать, что даже такое радикальное уменьшение размерности (с 5358 до 20!) позволит не только использовать получившиеся эмбеддинги в задаче ранжирования, но и наделит их \"суперспособностю\" к определению степени семантической близости, которой не обладали исходные sparse-эмбеддинги. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f128a108-3b91-454f-bbc3-bbc9f052fc5e",
   "metadata": {},
   "source": [
    "Посмотрим на получившиеся в результате SVD-разложения сингулярные значения:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "75155b0b-9639-434b-8484-e2699b152d5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13.21846562  7.81291588  6.39003008  6.30542922  6.02990338  5.84798858\n",
      "  5.59991578  5.54587373  5.36759762  5.15852419  5.07190324  4.82720005\n",
      "  4.75774718  4.66189747  4.60366438  4.57800603  4.51058406  4.47745504\n",
      "  4.36563922  4.36451438]\n"
     ]
    }
   ],
   "source": [
    "print(svd.singular_values_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46ee2493-0db5-4e6a-b3a1-5c2dd3f90fbc",
   "metadata": {},
   "source": [
    "Мы видим, что они убывают, как и должно быть, причем самый главный компонент выглядит особенно \"мощным\".\n",
    "\n",
    "Теперь попробуем получить матрицу термин-тематика (а не термин-документ как раньше). Для этого мы воспользуемся матрицей _Vt_ нашего разложения _X = USVt_.\n",
    "\n",
    "Обратите внимание, что _TfidfVectorizer_ выдал нам _транспонированную_ матрицу термин-документ, поэтому, в отличие от того варианта разложения которое мы разбирали на лекции, тут у нас \"все наоборот\", т.е. матрица _U_ соответствует документам, а матрица _Vt_ терминам."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "522a568d-4075-49ed-9068-834ec02a4956",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topic_0</th>\n",
       "      <th>topic_1</th>\n",
       "      <th>topic_2</th>\n",
       "      <th>topic_3</th>\n",
       "      <th>topic_4</th>\n",
       "      <th>topic_5</th>\n",
       "      <th>topic_6</th>\n",
       "      <th>topic_7</th>\n",
       "      <th>topic_8</th>\n",
       "      <th>topic_9</th>\n",
       "      <th>topic_10</th>\n",
       "      <th>topic_11</th>\n",
       "      <th>topic_12</th>\n",
       "      <th>topic_13</th>\n",
       "      <th>topic_14</th>\n",
       "      <th>topic_15</th>\n",
       "      <th>topic_16</th>\n",
       "      <th>topic_17</th>\n",
       "      <th>topic_18</th>\n",
       "      <th>topic_19</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>yup</th>\n",
       "      <td>0.003378</td>\n",
       "      <td>0.002858</td>\n",
       "      <td>-0.002525</td>\n",
       "      <td>-0.002105</td>\n",
       "      <td>-0.001963</td>\n",
       "      <td>-0.000207</td>\n",
       "      <td>-0.005215</td>\n",
       "      <td>0.000469</td>\n",
       "      <td>0.000207</td>\n",
       "      <td>0.000653</td>\n",
       "      <td>-0.001189</td>\n",
       "      <td>-0.002635</td>\n",
       "      <td>-0.002220</td>\n",
       "      <td>-0.003464</td>\n",
       "      <td>0.001621</td>\n",
       "      <td>0.001621</td>\n",
       "      <td>0.002894</td>\n",
       "      <td>0.000645</td>\n",
       "      <td>-0.002918</td>\n",
       "      <td>0.007704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zero</th>\n",
       "      <td>0.005956</td>\n",
       "      <td>-0.000196</td>\n",
       "      <td>-0.003918</td>\n",
       "      <td>-0.003631</td>\n",
       "      <td>0.001438</td>\n",
       "      <td>0.001204</td>\n",
       "      <td>-0.000401</td>\n",
       "      <td>-0.004462</td>\n",
       "      <td>-0.000463</td>\n",
       "      <td>0.005168</td>\n",
       "      <td>-0.002806</td>\n",
       "      <td>0.000237</td>\n",
       "      <td>0.011480</td>\n",
       "      <td>-0.001789</td>\n",
       "      <td>0.004662</td>\n",
       "      <td>-0.002423</td>\n",
       "      <td>-0.002463</td>\n",
       "      <td>-0.000432</td>\n",
       "      <td>-0.003558</td>\n",
       "      <td>0.000163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zip</th>\n",
       "      <td>0.009843</td>\n",
       "      <td>-0.025317</td>\n",
       "      <td>0.010224</td>\n",
       "      <td>0.014123</td>\n",
       "      <td>0.011043</td>\n",
       "      <td>-0.018178</td>\n",
       "      <td>-0.007631</td>\n",
       "      <td>-0.036915</td>\n",
       "      <td>-0.011516</td>\n",
       "      <td>-0.007796</td>\n",
       "      <td>-0.020277</td>\n",
       "      <td>-0.021518</td>\n",
       "      <td>-0.037796</td>\n",
       "      <td>0.040020</td>\n",
       "      <td>0.017798</td>\n",
       "      <td>0.061394</td>\n",
       "      <td>-0.006429</td>\n",
       "      <td>0.012478</td>\n",
       "      <td>0.051588</td>\n",
       "      <td>-0.051003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zone</th>\n",
       "      <td>0.006381</td>\n",
       "      <td>0.003135</td>\n",
       "      <td>-0.007423</td>\n",
       "      <td>-0.005353</td>\n",
       "      <td>-0.001497</td>\n",
       "      <td>-0.003252</td>\n",
       "      <td>0.000903</td>\n",
       "      <td>-0.003632</td>\n",
       "      <td>-0.003073</td>\n",
       "      <td>-0.013344</td>\n",
       "      <td>0.003078</td>\n",
       "      <td>0.005641</td>\n",
       "      <td>0.010431</td>\n",
       "      <td>-0.004679</td>\n",
       "      <td>-0.008713</td>\n",
       "      <td>0.008451</td>\n",
       "      <td>0.002264</td>\n",
       "      <td>0.012437</td>\n",
       "      <td>-0.005999</td>\n",
       "      <td>-0.008993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zoom</th>\n",
       "      <td>0.002315</td>\n",
       "      <td>-0.003965</td>\n",
       "      <td>-0.001239</td>\n",
       "      <td>-0.000660</td>\n",
       "      <td>-0.002567</td>\n",
       "      <td>0.000906</td>\n",
       "      <td>0.006911</td>\n",
       "      <td>-0.000178</td>\n",
       "      <td>-0.003172</td>\n",
       "      <td>0.004746</td>\n",
       "      <td>0.003572</td>\n",
       "      <td>-0.002368</td>\n",
       "      <td>-0.000511</td>\n",
       "      <td>0.000170</td>\n",
       "      <td>-0.003110</td>\n",
       "      <td>-0.005779</td>\n",
       "      <td>0.001702</td>\n",
       "      <td>0.003213</td>\n",
       "      <td>0.003137</td>\n",
       "      <td>-0.002417</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       topic_0   topic_1   topic_2   topic_3   topic_4   topic_5   topic_6  \\\n",
       "yup   0.003378  0.002858 -0.002525 -0.002105 -0.001963 -0.000207 -0.005215   \n",
       "zero  0.005956 -0.000196 -0.003918 -0.003631  0.001438  0.001204 -0.000401   \n",
       "zip   0.009843 -0.025317  0.010224  0.014123  0.011043 -0.018178 -0.007631   \n",
       "zone  0.006381  0.003135 -0.007423 -0.005353 -0.001497 -0.003252  0.000903   \n",
       "zoom  0.002315 -0.003965 -0.001239 -0.000660 -0.002567  0.000906  0.006911   \n",
       "\n",
       "       topic_7   topic_8   topic_9  topic_10  topic_11  topic_12  topic_13  \\\n",
       "yup   0.000469  0.000207  0.000653 -0.001189 -0.002635 -0.002220 -0.003464   \n",
       "zero -0.004462 -0.000463  0.005168 -0.002806  0.000237  0.011480 -0.001789   \n",
       "zip  -0.036915 -0.011516 -0.007796 -0.020277 -0.021518 -0.037796  0.040020   \n",
       "zone -0.003632 -0.003073 -0.013344  0.003078  0.005641  0.010431 -0.004679   \n",
       "zoom -0.000178 -0.003172  0.004746  0.003572 -0.002368 -0.000511  0.000170   \n",
       "\n",
       "      topic_14  topic_15  topic_16  topic_17  topic_18  topic_19  \n",
       "yup   0.001621  0.001621  0.002894  0.000645 -0.002918  0.007704  \n",
       "zero  0.004662 -0.002423 -0.002463 -0.000432 -0.003558  0.000163  \n",
       "zip   0.017798  0.061394 -0.006429  0.012478  0.051588 -0.051003  \n",
       "zone -0.008713  0.008451  0.002264  0.012437 -0.005999 -0.008993  \n",
       "zoom -0.003110 -0.005779  0.001702  0.003213  0.003137 -0.002417  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Vt = svd.components_.T\n",
    "term2topic = pd.DataFrame(data=Vt, index=vocab, columns = [f'topic_{r}' for r in range(0, Vt.shape[1])])\n",
    "term2topic.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00f6817a-e32b-4c3c-9b83-26ab2bc949ae",
   "metadata": {},
   "source": [
    "Мы видим, насколько каждый термин соответсвует каждой из 20 тематик.\n",
    "\n",
    "Возьмем теперь 0-ю (самую сильную, т.е. с самым большим сингулярным значением) тематику и посмотрим, какие термины наиболее тестно с ней связаны, т.е. по сути получим облако слов, лучше всего описывающих данную тематику:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "38879849-98d9-4049-888d-bde00c231d64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "don       0.163821\n",
       "like      0.161092\n",
       "just      0.160932\n",
       "know      0.159600\n",
       "people    0.151956\n",
       "think     0.136384\n",
       "does      0.127629\n",
       "use       0.114281\n",
       "good      0.113563\n",
       "time      0.109619\n",
       "Name: topic_0, dtype: float64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "concept_series = term2topic[f'topic_0']\n",
    "concept_series = concept_series.sort_values(ascending=False)\n",
    "concept_series[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "156b8319-91b0-40ae-8d6c-5255035f789b",
   "metadata": {},
   "source": [
    "Признаемся честно, тут ничего не понятно :-)\n",
    "\n",
    "Однако не будем расстраиваться раньше времени и посмотрим на следующую по силе тематику:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1942e4a6-83c3-4a5c-acec-b8a102237a52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "god        0.253677\n",
       "people     0.188681\n",
       "jesus      0.117584\n",
       "think      0.107536\n",
       "say        0.090334\n",
       "believe    0.085182\n",
       "don        0.084835\n",
       "bible      0.072719\n",
       "did        0.070173\n",
       "said       0.070052\n",
       "Name: topic_1, dtype: float64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "concept_series = term2topic[f'topic_1']\n",
    "concept_series = concept_series.sort_values(ascending=False)\n",
    "concept_series[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16904a84-ff12-4fea-8e6b-1816f865eb8c",
   "metadata": {},
   "source": [
    "Видим, что у нас отчетливо выделилась тематика, связанная с религией, что неудивительно с учетом того, что в датасете были использованы сообщения из каналов _alt.atheism_, _soc.religion.christian_ и _talk.religion.misc_!\n",
    "\n",
    "Посмотрим еще на какую-нибудь тематику:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "50cdc15a-fbdd-4099-a0f4-fa1adbdf6d75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "drive         0.436100\n",
       "scsi          0.240459\n",
       "drives        0.134428\n",
       "disk          0.133809\n",
       "key           0.128115\n",
       "hard          0.127249\n",
       "ide           0.124890\n",
       "controller    0.119298\n",
       "chip          0.116587\n",
       "card          0.108599\n",
       "Name: topic_5, dtype: float64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "concept_series = term2topic[f'topic_5']\n",
    "concept_series = concept_series.sort_values(ascending=False)\n",
    "concept_series[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dde08e9f-4f1e-4387-ba53-c951229a4dd9",
   "metadata": {},
   "source": [
    "Тут у нас, очевидно, что-то околокомпьютерное.\n",
    "\n",
    "Таким образом, мы видим, что метод LSA действительно позволяет выделять осмысленные тематики, несмотря на то, что мы использовали эмбеддинги размером всего 20 компонент!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "817b7b99-bbc9-485b-b5d8-d2b6c6837ae5",
   "metadata": {},
   "source": [
    "## Ранжирование с помощью LSA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f0ed057-3167-4b04-a430-065c65a07495",
   "metadata": {},
   "source": [
    "Теперь перейдем к самому интересному: попробуем ранжировать наши документы, используя в качестве ранков косинусное расстояние между LSA-векторами запросов и документов, и сравним с тем что получилось бы с использованием обычных TF-IDF-векторов."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc9f6447-819e-4b3f-aaf4-550d164c49d4",
   "metadata": {},
   "source": [
    "Допустим, у нас есть текстовый запрос query.\n",
    "\n",
    "Напишем две фукнции _search_sparse(query)_ и _search_dense(query)_, которые находят топ-К ближайших к запросу документов с использованием, соответственно, TF-IDF и LSA векторов.\n",
    "\n",
    "Начнем с _search_sparse(query)_, она:\n",
    "- векторизует наш запрос с помощью обученного ранее векторизатора\n",
    "- считает попарную близость между TF-IDF-вектором запроса и TF-IDF-векторами документов\n",
    "- выводит на экран запрос и ранжированный список документов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "25474285-aa1f-4165-9fec-fda69f426d47",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_sparse(query):\n",
    "        # Vectorize query\n",
    "        X_query = vectorizer.transform([query])\n",
    "        print(f\"vectorized query: X_query.shape = {X_query.shape}\")\n",
    "\n",
    "        # Query-docs similarity\n",
    "        S = pairwise.cosine_similarity(X_query, X_docs)\n",
    "        print(f\"got similarities: S.shape = {S.shape}\")\n",
    "\n",
    "        # Rank docs\n",
    "        scores = S[0]\n",
    "        indexes = np.argsort(scores)[::-1]\n",
    "        ranked_docs = np.array(docs)[indexes]\n",
    "        ranked_doc_scores = scores[indexes]\n",
    "\n",
    "        # Output query and list of ranked docs\n",
    "        print(f\"query = '{query}'\")\n",
    "        for i, doc in enumerate(ranked_docs[0:3]):\n",
    "            score = ranked_doc_scores[i]\n",
    "            print(f\"SPARSE: [{i}]: doc = '{doc}' score = {score:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4d188b4-6ebf-4fb7-9c92-0db263049140",
   "metadata": {},
   "source": [
    "И, аналогично, напишем функцию _search_dense(query)_, которая делает все то же самое, но уже в пространстве LSA-эмбеддингов:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d933bae5-563f-4581-9e22-989976e0448d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_dense(query):\n",
    "        # Vectorize query\n",
    "        X_query = vectorizer.transform([query])\n",
    "        print(f\"vectorized query: X_query.shape = {X_query.shape}\")\n",
    "\n",
    "        # Embed query\n",
    "        E_query = svd.transform(X_query)\n",
    "        print(f\"SVD-vectorized query: E_query.shape = {E_query.shape}\")\n",
    "\n",
    "        # Query-docs similarity\n",
    "        S = pairwise.cosine_similarity(E_query, E_docs)\n",
    "        print(f\"got latent similarities: S.shape = {S.shape}\")\n",
    "\n",
    "        # Rank docs\n",
    "        scores = S[0]\n",
    "        indexes = np.argsort(scores)[::-1]\n",
    "        ranked_docs = np.array(docs)[indexes]\n",
    "        ranked_doc_scores = scores[indexes]\n",
    "\n",
    "        # Output query and list of ranked docs\n",
    "        print(f\"query = '{query}'\")\n",
    "        for i, doc in enumerate(ranked_docs[0:3]):\n",
    "            score = ranked_doc_scores[i]\n",
    "            print(f\"DENSE: [{i}]: doc = '{doc}' score = {score:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d4804e3-3717-497f-b156-fa77bbd64cdb",
   "metadata": {},
   "source": [
    "Применим наши функции к запросу \"mars\", начем со sparse-варианта:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c67b5fe8-5eb2-410b-8409-661f2d675721",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vectorized query: X_query.shape = (1, 5358)\n",
      "got similarities: S.shape = (1, 11314)\n",
      "query = 'mars'\n",
      "SPARSE: [0]: doc = 'What is the deal with life on Mars?  I save the \"face\" and heard \n",
      "associated theories. (which sound thin to me)\n",
      "\n",
      "Are we going back to Mars to look at this face agian?\n",
      "Does anyone buy all the life theories?\n",
      "' score = 0.514\n",
      "SPARSE: [1]: doc = '\n",
      "The \"face\" is an accident of light and shadow.  There are many \"faces\" in\n",
      "landforms on Earth; none is artificial (well, excluding Mount Rushmore and\n",
      "the like...).  There is also a smiley face on Mars, and a Kermit The Frog.\n",
      "\n",
      "The question of life in a more mundane sense -- bacteria or the like -- is\n",
      "not quite closed, although the odds are against it, and the most that the\n",
      "more orthodox exobiologists are hoping for now is fossils.\n",
      "\n",
      "There are currently no particular plans to do any further searches for life.\n",
      "\n",
      "\n",
      "Mars Observer, currently approaching Mars, will probably try to get a better\n",
      "image or two of the \"face\" at some point.  It's not high priority; nobody\n",
      "takes it very seriously.  The shadowed half of the face does not look very\n",
      "face-like, so all it will take is one shot at a different sun angle to ruin\n",
      "the illusion.' score = 0.438\n",
      "SPARSE: [2]: doc = '\n",
      "Not quite true.  One of the instruments on Mars Observer will be searching\n",
      "for potential fossil sites.   \n",
      "' score = 0.410\n"
     ]
    }
   ],
   "source": [
    "search_sparse(\"mars\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcc06989-a97a-4b93-9239-a5869af19226",
   "metadata": {},
   "source": [
    "Видим, что в топе выдачи вполне себе релевантные документы про планету Марс, как и ожидалось.\n",
    "\n",
    "Но что же выдаст dense-вариант?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "71a9bfe1-5817-4786-b227-0b714a2f8ba1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vectorized query: X_query.shape = (1, 5358)\n",
      "SVD-vectorized query: E_query.shape = (1, 20)\n",
      "got latent similarities: S.shape = (1, 11314)\n",
      "query = 'mars'\n",
      "DENSE: [0]: doc = 'Other idea for old space crafts is as navigation beacons and such..\n",
      "Why not?? If you can put them on \"safe\" \"pause\" mode.. why not have them be\n",
      "activated by a signal from a space craft (manned?) to act as a naviagtion\n",
      "beacon, to take a directional plot on??' score = 0.982\n",
      "DENSE: [1]: doc = '\n",
      "If raw materials where to cost enough that getting them from space would\n",
      "be cost effective then the entire world economy would colapse long\n",
      "before the space mines could be built.\n",
      "\n",
      "  Allen\n",
      "' score = 0.981\n",
      "DENSE: [2]: doc = ': Announce that a reward of $1 billion would go to the first corporation \n",
      ": who successfully keeps at least 1 person alive on the moon for a year. \n",
      ": Then you'd see some of the inexpensive but not popular technologies begin \n",
      ": to be developed. THere'd be a different kind of space race then!\n",
      "\n",
      "I'm an advocate of this idea for funding Space Station work, and I\n",
      "throw around the $1 billion figure for that \"reward.\"  I suggest that\n",
      "you increase the Lunar reward to about $3 billion.\n",
      "\n",
      "This would encourage private industry to invest in space, which \n",
      "should be one of NASA's primary goals.\n",
      "\n",
      "-- Ken Jenks, NASA/JSC/GM2, Space Shuttle Program Office\n",
      "      kjenks@gothamcity.jsc.nasa.gov  (713) 483-4368' score = 0.978\n"
     ]
    }
   ],
   "source": [
    "search_dense(\"mars\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f5ed20b-5ae6-4f73-af54-0f09aa2dab93",
   "metadata": {},
   "source": [
    "Мы видим, что находятся документы про космические исследования, причем ни в одном из документов в топ-3 нет слова \"mars\"!\n",
    "\n",
    "Очевидно, что это не может быть случайностью, а это значит что мы действительно смогли сматчить запросы и документы не по факту наличия ключевых слов, а по их семантической близости!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a83a95e2-d7e7-4f5f-9183-99418bf8825c",
   "metadata": {},
   "source": [
    "Попробуем еще один запрос:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "aa2671ff-1077-493c-b257-ce1b2e8ed0e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vectorized query: X_query.shape = (1, 5358)\n",
      "got similarities: S.shape = (1, 11314)\n",
      "query = 'penguins'\n",
      "SPARSE: [0]: doc = 'How do you beat the Penguins?\n",
      "\n",
      "\n",
      "Crash the team plane.\n",
      "' score = 0.507\n",
      "SPARSE: [1]: doc = 'The subject line says it all.  Is it terribly difficult to get tickets\n",
      "to Penguins games, especially now that they are in the playoffs?  Would\n",
      "it be easy to find scalpers outside of the Igloo selling tickets?' score = 0.315\n",
      "SPARSE: [2]: doc = '\n",
      "Why?  I'm calling this Penguins ... in 6.  Only that with the way \n",
      "things stand, the only radio game at that hour is from the Devils\n",
      "on WABC, 770 AM.  It'd be nice to have a Sony Watchman, but ...\n",
      "\n",
      "No need to be paranoid, Robbie.  Don't judge me by my geographic\n",
      "coordinates ...\n",
      "\n",
      "Jets over Nordiques in the final ... 7.\n",
      "\n",
      "gld' score = 0.296\n"
     ]
    }
   ],
   "source": [
    "search_sparse(\"penguins\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8db9dae8-8695-4e11-94b1-9e175a0bf5ee",
   "metadata": {},
   "source": [
    "В топе документы про хоккей, т.к. _Pittsburgh Penguins_ -- это одна из комманд NHL.\n",
    "\n",
    "Повторим поиск с использованием LSA-эмбеддингов:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ea57a309-059e-4194-b819-44523bafd05e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vectorized query: X_query.shape = (1, 5358)\n",
      "SVD-vectorized query: E_query.shape = (1, 20)\n",
      "got latent similarities: S.shape = (1, 11314)\n",
      "query = 'penguins'\n",
      "DENSE: [0]: doc = '.\n",
      ".\n",
      ".\n",
      "\n",
      "ESPN had the Houston Astros @ Chicago Cubs game scheduled for last night on the\n",
      "west coast. \n",
      "\n",
      "Since the game was rained out, they showed the Toronto Maple Leafs at the\n",
      "Detroit Red Wings game instead.' score = 0.992\n",
      "DENSE: [1]: doc = '\n",
      "Not clear to me at all.  I'd certainly rather have a team who was winning\n",
      "4-1 games than 2-1 games.  In the 2-1 game, luck is going to play a much\n",
      "bigger role than in the 4-1 game. ' score = 0.987\n",
      "DENSE: [2]: doc = 'First game, first at bat.' score = 0.983\n"
     ]
    }
   ],
   "source": [
    "search_dense(\"penguins\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddeb6324-d4a0-458b-88a7-844420734574",
   "metadata": {},
   "source": [
    "На 1-м месте у нас тоже документ про хоккей (т.к. _Toronto Maple Leafs_ и _Detroit Red Wings_ -- это тоже команды NHL), на 2-м просто что-то похожее на хоккей, а вот 3-й документ уже кажется про бейсбол (судя по слову _bat_ т.е. \"бита\"), но это тоже спорт, т.е. и тут мы смогли распознать какие-то оттенки семантики."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
