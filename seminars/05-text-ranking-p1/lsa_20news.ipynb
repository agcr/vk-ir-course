{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c68b6411-668d-431f-9556-9a478597aa6c",
   "metadata": {},
   "source": [
    "# Латентно-семантический анализ (Latent Semantic Analysis или LSA)\n",
    "\n",
    "В этом тюториале мы:\n",
    "- познакомимся со стандартным датасетом _20 Newsgroups_\n",
    "- научимся использовать метод латентно-семантическго анализа (LSA) для представления текстов в виде плотных (dense) эмбеддингов, компонентами которых являются скрытые признаки-тематики\n",
    "- научимся использовать LSA-эмбеддинги для решения задач семантической близости и текстового ранжирования\n",
    "\n",
    "При подготовке этого тюториала использовались материалы из (отличной) книги _Natural Language Processing in Action_ (авторы _Hobson Lane_ и _Maria Dyshel_): https://www.goodreads.com/book/show/59694556-natural-language-processing-in-action-second-edition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f0a7027-7d31-480c-9e05-44b6ca966f68",
   "metadata": {},
   "source": [
    "Импортируем модули которые нам понадобятся впоследствии:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21fb783d-937e-40a6-88b5-3df49b614a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "from timeit import default_timer as timer\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import pairwise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23d43408-4e27-4eed-9cc2-c0839c707f92",
   "metadata": {},
   "source": [
    "## Датасет 20 Newgroups"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63ef0386-af81-4cd3-821f-b2a28d839171",
   "metadata": {},
   "source": [
    "Наш тюториал будет основан на классическом датасете _20 Newsgroups_.\n",
    "\n",
    "Этот датасет был собран еще в начале 90-х годов, и представляет из себя коллекцию из примерно 20000 текстовых сообщений, отправленных в 20 разных каналах (newsgroups) в сети Usenet (https://en.wikipedia.org/wiki/Usenet_newsgroup). Название канала (напр. _sci.space_, т.е. канал про космос) используется как метка класса.\n",
    "\n",
    "Другими словами, это небольшой текстовый датасет для мультиклассовой классификации, всего 20 классов (по числу каналов).\n",
    "\n",
    "Этот датасет приобрел очень большую популярность в мире NLP, и широко используется для иллюстрации работы различных алгоритмов, моделей и т.п.\n",
    "\n",
    "Кроме того, этим датасетом очень удобно пользоваться, т.к. он доступен \"из коробки\" в библиотеке _scikit-learn_: https://scikit-learn.org/stable/datasets/real_world.html#the-20-newsgroups-text-dataset\n",
    "\n",
    "Попробуем загрузить датасет:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70f8c76e-e7f9-43f3-9c0d-d9076d242a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter e-mail headers, signature blocks and quotes\n",
    "data_remove = ('headers', 'footers', 'quotes')\n",
    "\n",
    "# Fetcher options\n",
    "data_kwargs = { 'remove': data_remove, 'shuffle': True, 'random_state': 22 }\n",
    "\n",
    "# Fetch train subset. \n",
    "# Bunch contains:\n",
    "#   - data          -- list of text documents\n",
    "#   - filenames     -- list of sample filenames\n",
    "#   - target        -- numeric target in [0, 19]\n",
    "#   - target_names  -- list of target names\n",
    "train_bunch = datasets.fetch_20newsgroups(subset=\"train\", **data_kwargs)\n",
    "print(f\"fetched 20 newsgroups dataset: num_docs = {len(train_bunch.data)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "457e6d90-1111-40cd-a2ce-f2a6a2bc580e",
   "metadata": {},
   "source": [
    "Обратите внимание, что:\n",
    "- датасет поделен на train и test части, нас дальше будет интересовать только train\n",
    "- мы скачиваем очищенный вариант датасета с удаленной из сообщений служебной информацией, такой как заголовки писем и т.п.\n",
    "\n",
    "Всего у нас скачалось ~11 тыс. документов.\n",
    "\n",
    "Посмотрим теперь на метки классов:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d3494f5-16e7-49cd-9c71-ca3f85cecaae",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_bunch.target)\n",
    "print(train_bunch.target_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fed1494-3cf6-4452-bc8a-c36cd025fc72",
   "metadata": {},
   "source": [
    "Видно, что таргеты доступны как в виде текстов (названий каналов), так и в виде числовых идентификаторов.\n",
    "\n",
    "Посмотрим теперь на типичные тексты сообщений:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e5a0d16-67f5-4653-8dc6-670210a2b929",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = train_bunch.data\n",
    "print(docs[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07def150-f9b2-4d2d-8bb8-01b526dc08b2",
   "metadata": {},
   "source": [
    "К какому классу относится наше сообщение?\n",
    "\n",
    "Это легко понять следующим образом:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "577b554b-0129-4ca5-86c7-58e330b41a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_0 = train_bunch.target[0]\n",
    "print(train_bunch.target_names[y_0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f27cfed-e8b4-46c8-9897-a54c6d79f794",
   "metadata": {},
   "source": [
    "Как (наверное) нетрудно было догадаться, это сообщение про хоккей, отправленное в канал _rec.sport.hockey_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aee774c-e382-43e4-9b26-16e9190c309d",
   "metadata": {},
   "source": [
    "## LSA-эмбеддинги"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c5e1518-ac81-456e-97f3-9bffee59ffa3",
   "metadata": {},
   "source": [
    "Попробуем теперь воспользоваться методом LSA и получить эмбеддинги наших текстов.\n",
    "\n",
    "Первым делом надо превратить текст в матрицу термин-документ, т.е. получить для него разреженное признаковое представление.<br>\n",
    "Сразу обратим внимание, что в качестве весов в ячейках матрицы термин-документ можно бы было использовать любой из методов взвешивания, изученных нами в лекции про модели векторого пространства, например:\n",
    "- бинарные веса 1 (слово есть в документе) или 0 (слова нет в документе), которые проще всего получить с помощью класса _CountVectorizer_ из библиотеки _scikit-learn_\n",
    "- TFы, т.е. частоты слова в документах\n",
    "- TF-IDFы с разными способами нормализации, сглаживания и т.п.\n",
    "\n",
    "На практике, как правило, в качестве признаков лучше всего работают TF-IDFы, поэтому мы дальше будем использовать именно их.<br>\n",
    "Векторизуем наши тексты с помощью класса _TfidfVectorizer_:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67fb4cd6-6a57-4f33-81c5-74bf492aef01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare vectorizer\n",
    "vectorizer = TfidfVectorizer(min_df=0.002, max_df=0.5, stop_words='english', decode_error='ignore')\n",
    "\n",
    "# Vectorize\n",
    "start = timer()\n",
    "X_docs = vectorizer.fit_transform(docs)\n",
    "print(type(X_docs))\n",
    "print(f\"vectorized: X_docs.shape = {X_docs.shape} elapsed = {timer() - start:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ab54e72-f327-4808-b41c-fa05514bf3ae",
   "metadata": {},
   "source": [
    "При векторизации мы сразу выкидываем стоп-слова (параметр _stop_words_), а также слишком редкие (параметр _min_df_) и слишком часто встречающиеся слова (_max_df_).\n",
    "\n",
    "В результате у нас получилась матрица термин-документ размером 11314x5358, т.е. 11314 документов и 5358 терминов-слов.\n",
    "\n",
    "Обратим внимание, что при желании мы могли бы использовать тут стемминг, лемматизацию или какую-то кастомную токенизацию, но не делаем этого чтобы не загромождать тюториал.\n",
    "\n",
    "Наша матрица содержит TF-IDF весах, причем все векторы документы нормализованы с использованием L2-нормы (это дефолтное поведение векторизатора):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "644721a6-82ff-4a06-89ab-db2a58f3b249",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_docs.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "697d0810-0356-47d3-84ff-91823c0700d6",
   "metadata": {},
   "source": [
    "Видно, что матрица крайне разреженная, и действительно:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6002a6b2-a141-43b5-87cd-87138ef0b1c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_docs.nnz)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3701a8ce-798a-4eb5-88fa-aa5495066240",
   "metadata": {},
   "source": [
    "Т.е. у нас заполнено только 516701 / (11314 * 5358) = 0.0085 элементов!\n",
    "\n",
    "Посмотрим сразу и на термины, которые использует векторизатор:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e57ed38-282a-4e35-95fe-13e53f5f0b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = vectorizer.get_feature_names_out()\n",
    "docs_df = pd.DataFrame(X_docs.toarray(), columns=vocab)\n",
    "docs_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fae6d4dc-90c1-4d87-bf1d-65d31dd1824e",
   "metadata": {},
   "source": [
    "Теперь начинается самое интересное.\n",
    "\n",
    "Применим к нашей матрице SVD-разложение:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b6d9cf8-0699-4ce0-ac11-8c4fd756c165",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare SVD\n",
    "svd = TruncatedSVD(n_components=20, n_iter=100, random_state=22)\n",
    "\n",
    "# Run SVD\n",
    "start = timer()\n",
    "E_docs = svd.fit_transform(X_docs)\n",
    "print(E_docs.round(3))\n",
    "print(f\"decomposed: E_docs.shape = {E_docs.shape} elapsed = {timer() - start:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afee7523-8a85-4d2d-8f1e-5693b00637fb",
   "metadata": {},
   "source": [
    "Мы используем класс _TruncatedSVD_ из библиотеки _scikit-learn_, а в качестве параметра _n_components_ передаем ему желаемый размер нашего скрытого пространства.\n",
    "\n",
    "Размер скрытого пространства является, по сути, гиперпараметром нашего метода и его можно попробовать подобрать под целевую задачу, в которой мы впоследствии будем использовать наши эмбеддинги. На практике, как правило, хорошо работают эмбеддинги размером 100-300 элементов.\n",
    "\n",
    "В данном примере мы будем использовать _n_components_ равное 20, потому что:\n",
    "- нам заранее известно, что у нас всего 20 классов, т.е. кажется что 20 -- это минимальное количество тематик, которыми можно описать нашу коллекцию\n",
    "- с другой стороны, хотим показать, что даже такое радикальное уменьшение размерности (с 5358 до 20!) позволит не только использовать получившиеся эмбеддинги в задаче ранжирования, но и наделит их \"суперспособностю\" к определению степени семантической близости, которой не обладали исходные sparse-эмбеддинги. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f128a108-3b91-454f-bbc3-bbc9f052fc5e",
   "metadata": {},
   "source": [
    "Посмотрим на получившиеся в результате SVD-разложения сингулярные значения:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75155b0b-9639-434b-8484-e2699b152d5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(svd.singular_values_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46ee2493-0db5-4e6a-b3a1-5c2dd3f90fbc",
   "metadata": {},
   "source": [
    "Мы видим, что они убывают, как и должно быть, причем самый главный компонент выглядит особенно \"мощным\".\n",
    "\n",
    "Теперь попробуем получить матрицу термин-тематика (а не термин-документ как раньше). Для этого мы воспользуемся матрицей _Vt_ нашего разложения _X = USVt_.\n",
    "\n",
    "Обратите внимание, что _TfidfVectorizer_ выдал нам _транспонированную_ матрицу термин-документ, поэтому, в отличие от того варианта разложения которое мы разбирали на лекции, тут у нас \"все наоборот\", т.е. матрица _U_ соответствует документам, а матрица _Vt_ терминам."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "522a568d-4075-49ed-9068-834ec02a4956",
   "metadata": {},
   "outputs": [],
   "source": [
    "Vt = svd.components_.T\n",
    "term2topic = pd.DataFrame(data=Vt, index=vocab, columns = [f'topic_{r}' for r in range(0, Vt.shape[1])])\n",
    "term2topic.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00f6817a-e32b-4c3c-9b83-26ab2bc949ae",
   "metadata": {},
   "source": [
    "Мы видим, насколько каждый термин соответсвует каждой из 20 тематик.\n",
    "\n",
    "Возьмем теперь 0-ю (самую сильную, т.е. с самым большим сингулярным значением) тематику и посмотрим, какие термины наиболее тестно с ней связаны, т.е. по сути получим облако слов, лучше всего описывающих данную тематику:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38879849-98d9-4049-888d-bde00c231d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "concept_series = term2topic[f'topic_0']\n",
    "concept_series = concept_series.sort_values(ascending=False)\n",
    "concept_series[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "156b8319-91b0-40ae-8d6c-5255035f789b",
   "metadata": {},
   "source": [
    "Признаемся честно, тут ничего не понятно :-)\n",
    "\n",
    "Однако не будем расстраиваться раньше времени и посмотрим на следующую по силе тематику:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1942e4a6-83c3-4a5c-acec-b8a102237a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "concept_series = term2topic[f'topic_1']\n",
    "concept_series = concept_series.sort_values(ascending=False)\n",
    "concept_series[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16904a84-ff12-4fea-8e6b-1816f865eb8c",
   "metadata": {},
   "source": [
    "Видим, что у нас отчетливо выделилась тематика, связанная с религией, что неудивительно с учетом того, что в датасете были использованы сообщения из каналов _alt.atheism_, _soc.religion.christian_ и _talk.religion.misc_!\n",
    "\n",
    "Посмотрим еще на какую-нибудь тематику:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50cdc15a-fbdd-4099-a0f4-fa1adbdf6d75",
   "metadata": {},
   "outputs": [],
   "source": [
    "concept_series = term2topic[f'topic_5']\n",
    "concept_series = concept_series.sort_values(ascending=False)\n",
    "concept_series[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dde08e9f-4f1e-4387-ba53-c951229a4dd9",
   "metadata": {},
   "source": [
    "Тут у нас, очевидно, что-то околокомпьютерное.\n",
    "\n",
    "Таким образом, мы видим, что метод LSA действительно позволяет выделять осмысленные тематики, несмотря на то, что мы использовали эмбеддинги размером всего 20 компонент!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "817b7b99-bbc9-485b-b5d8-d2b6c6837ae5",
   "metadata": {},
   "source": [
    "## Ранжирование с помощью LSA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f0ed057-3167-4b04-a430-065c65a07495",
   "metadata": {},
   "source": [
    "Теперь перейдем к самому интересному: попробуем ранжировать наши документы, используя в качестве ранков косинусное расстояние между LSA-векторами запросов и документов, и сравним с тем что получилось бы с использованием обычных TF-IDF-векторов."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc9f6447-819e-4b3f-aaf4-550d164c49d4",
   "metadata": {},
   "source": [
    "Допустим, у нас есть текстовый запрос query.\n",
    "\n",
    "Напишем две фукнции _search_sparse(query)_ и _search_dense(query)_, которые находят топ-К ближайших к запросу документов с использованием, соответственно, TF-IDF и LSA векторов.\n",
    "\n",
    "Начнем с _search_sparse(query)_, она:\n",
    "- векторизует наш запрос с помощью обученного ранее векторизатора\n",
    "- считает попарную близость между TF-IDF-вектором запроса и TF-IDF-векторами документов\n",
    "- выводит на экран запрос и ранжированный список документов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25474285-aa1f-4165-9fec-fda69f426d47",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_sparse(query):\n",
    "        # Vectorize query\n",
    "        X_query = vectorizer.transform([query])\n",
    "        print(f\"vectorized query: X_query.shape = {X_query.shape}\")\n",
    "\n",
    "        # Query-docs similarity\n",
    "        S = pairwise.cosine_similarity(X_query, X_docs)\n",
    "        print(f\"got similarities: S.shape = {S.shape}\")\n",
    "\n",
    "        # Rank docs\n",
    "        scores = S[0]\n",
    "        indexes = np.argsort(scores)[::-1]\n",
    "        ranked_docs = np.array(docs)[indexes]\n",
    "        ranked_doc_scores = scores[indexes]\n",
    "\n",
    "        # Output query and list of ranked docs\n",
    "        print(f\"query = '{query}'\")\n",
    "        for i, doc in enumerate(ranked_docs[0:3]):\n",
    "            score = ranked_doc_scores[i]\n",
    "            print(f\"SPARSE: [{i}]: doc = '{doc}' score = {score:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4d188b4-6ebf-4fb7-9c92-0db263049140",
   "metadata": {},
   "source": [
    "И, аналогично, напишем функцию _search_dense(query)_, которая делает все то же самое, но уже в пространстве LSA-эмбеддингов:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d933bae5-563f-4581-9e22-989976e0448d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_dense(query):\n",
    "        # Vectorize query\n",
    "        X_query = vectorizer.transform([query])\n",
    "        print(f\"vectorized query: X_query.shape = {X_query.shape}\")\n",
    "\n",
    "        # Embed query\n",
    "        E_query = svd.transform(X_query)\n",
    "        print(f\"SVD-vectorized query: E_query.shape = {E_query.shape}\")\n",
    "\n",
    "        # Query-docs similarity\n",
    "        S = pairwise.cosine_similarity(E_query, E_docs)\n",
    "        print(f\"got latent similarities: S.shape = {S.shape}\")\n",
    "\n",
    "        # Rank docs\n",
    "        scores = S[0]\n",
    "        indexes = np.argsort(scores)[::-1]\n",
    "        ranked_docs = np.array(docs)[indexes]\n",
    "        ranked_doc_scores = scores[indexes]\n",
    "\n",
    "        # Output query and list of ranked docs\n",
    "        print(f\"query = '{query}'\")\n",
    "        for i, doc in enumerate(ranked_docs[0:3]):\n",
    "            score = ranked_doc_scores[i]\n",
    "            print(f\"DENSE: [{i}]: doc = '{doc}' score = {score:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d4804e3-3717-497f-b156-fa77bbd64cdb",
   "metadata": {},
   "source": [
    "Применим наши функции к запросу \"mars\", начем со sparse-варианта:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c67b5fe8-5eb2-410b-8409-661f2d675721",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_sparse(\"mars\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcc06989-a97a-4b93-9239-a5869af19226",
   "metadata": {},
   "source": [
    "Видим, что в топе выдачи вполне себе релевантные документы про планету Марс, как и ожидалось.\n",
    "\n",
    "Но что же выдаст dense-вариант?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71a9bfe1-5817-4786-b227-0b714a2f8ba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_dense(\"mars\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f5ed20b-5ae6-4f73-af54-0f09aa2dab93",
   "metadata": {},
   "source": [
    "Мы видим, что находятся документы про космические исследования, причем ни в одном из документов в топ-3 нет слова \"mars\"!\n",
    "\n",
    "Очевидно, что это не может быть случайностью, а это значит что мы действительно смогли сматчить запросы и документы не по факту наличия ключевых слов, а по их семантической близости!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a83a95e2-d7e7-4f5f-9183-99418bf8825c",
   "metadata": {},
   "source": [
    "Попробуем еще один запрос:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa2671ff-1077-493c-b257-ce1b2e8ed0e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_sparse(\"penguins\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8db9dae8-8695-4e11-94b1-9e175a0bf5ee",
   "metadata": {},
   "source": [
    "В топе документы про хоккей, т.к. _Pittsburgh Penguins_ -- это одна из комманд NHL.\n",
    "\n",
    "Повторим поиск с использованием LSA-эмбеддингов:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea57a309-059e-4194-b819-44523bafd05e",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_dense(\"penguins\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddeb6324-d4a0-458b-88a7-844420734574",
   "metadata": {},
   "source": [
    "На 1-м месте у нас тоже документ про хоккей (т.к. _Toronto Maple Leafs_ и _Detroit Red Wings_ -- это тоже команды NHL), на 2-м просто что-то похожее на хоккей, а вот 3-й документ уже кажется про бейсбол (судя по слову _bat_ т.е. \"бита\"), но это тоже спорт, т.е. и тут мы смогли распознать какие-то оттенки семантики."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
